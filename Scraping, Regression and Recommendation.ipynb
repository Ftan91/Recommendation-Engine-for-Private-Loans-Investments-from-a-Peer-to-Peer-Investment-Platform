{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, ElementClickInterceptedException\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Note_ID', 'Grade', 'Rate_%', 'Effective_Rate_%', 'Term', 'PD_%', 'Funding', 'Funded_%']\n",
    "df = pd.DataFrame(None,columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin scraping all of the available loan data and whilst we are gathering the data, also conduct some basic string / text manipulation to get the loan data into a desirable DataFrame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages scrapped: 1\n",
      "Pages scrapped: 2\n",
      "Pages scrapped: 3\n",
      "Pages scrapped: 4\n",
      "Pages scrapped: 5\n",
      "Pages scrapped: 6\n",
      "Pages scrapped: 7\n",
      "Pages scrapped: 8\n",
      "Pages scrapped: 9\n",
      "Pages scrapped: 10\n",
      "Pages scrapped: 11\n",
      "Pages scrapped: 12\n",
      "Pages scrapped: 13\n",
      "Pages scrapped: 14\n",
      "Pages scrapped: 15\n",
      "Pages scrapped: 16\n",
      "Pages scrapped: 17\n",
      "Pages scrapped: 18\n",
      "Pages scrapped: 19\n",
      "Pages scrapped: 20\n",
      "Pages scrapped: 21\n",
      "Pages scrapped: 22\n",
      "Pages scrapped: 23\n",
      "Pages scrapped: 24\n",
      "Pages scrapped: 25\n",
      "Pages scrapped: 26\n",
      "Pages scrapped: 27\n",
      "Pages scrapped: 28\n",
      "Pages scrapped: 29\n",
      "Pages scrapped: 30\n",
      "Pages scrapped: 31\n",
      "Pages scrapped: 32\n",
      "Pages scrapped: 33\n",
      "Pages scrapped: 34\n",
      "Pages scrapped: 35\n",
      "Pages scrapped: 36\n",
      "Pages scrapped: 37\n",
      "Pages scrapped: 38\n",
      "Pages scrapped: 39\n",
      "Pages scrapped: 40\n"
     ]
    }
   ],
   "source": [
    "url = 'https://p2p.fundaztic.com/loaninfo/openHistoryLoan.htm'\n",
    "driver = webdriver.Chrome('/Users/weeliptan/Desktop/chromedriver')\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    table_elements = driver.find_elements_by_xpath(\"//*[@id='loaninfotr']\")\n",
    "    for element in table_elements:\n",
    "        word_list = element.text.split()\n",
    "        word_list[3] = word_list[3].replace('%','')\n",
    "        word_list[-1] = word_list[-1].replace('%','')\n",
    "        word_list[1] = word_list[1] + word_list[2]\n",
    "        word_list.pop(2), word_list.pop(5)\n",
    "        array = np.array(word_list)\n",
    "        word_list_df = pd.DataFrame(array.reshape(1,-1), columns = df.columns)\n",
    "        df = df.append(word_list_df, ignore_index=True)\n",
    "\n",
    "    try:\n",
    "        next_link = driver.find_elements_by_xpath(\"//*[@id='bot_next_page']\")\n",
    "        next_link[-1].click()\n",
    "        i += 1\n",
    "        print ('Pages scrapped:', i)\n",
    "        time.sleep(2)\n",
    "\n",
    "    except (NoSuchElementException, StaleElementReferenceException, ElementClickInterceptedException):\n",
    "        i += 1\n",
    "        print ('Pages scrapped:', i)\n",
    "        print ('Completed')\n",
    "        break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took a while. Let's save this into a csv for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Latest_Scrapped_P2P_Notes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning, pre-processing and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the shape of the scrapped data and display the first 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.read_csv('Latest_Scrapped_P2P_Notes.csv', index_col = 0)\n",
    "\n",
    "display(df_summary.shape)\n",
    "df_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the data worked as expected. Now we check the datatype of each column and for any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fix the text number formatting ahead of converting it into float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.Funding = df_summary.Funding.apply(lambda x: x.replace(',', ''))\n",
    "df_summary.Grade = df_summary.Grade.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert non-float64 columns to float64 which will be required for future numerical tasks and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [col for col in df_summary.columns if col not in ['Rate_%','Effective_Rate_%', 'PD_%', 'Funded_%']] \n",
    "columns_to_convert\n",
    "\n",
    "for i in columns_to_convert:\n",
    "    df_summary[i] = df_summary[i].astype('float64')\n",
    "    \n",
    "df_summary.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = df_summary.iloc[:, 1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by glancing at the pairplot above, we can deduct that there are some form of linear relationship between our target variable here 'Rate_%' and other features here including the 'PD_%', 'Funding' and 'Term'. Just so we include only succesfully funded notes, let's exclude notes which were not succesfully disbursed (ie < 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = df_summary[df_summary['Funded_%'] < 100].index\n",
    "df_summary.drop(rows_to_drop, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, let's now split our data into X and Y. For X we drop:\n",
    " - Note_ID: this does not provide any predictive value\n",
    " - Funded_%: as we've now dropped anything below 100%, this would be removed as the variance is 0 and would not add much predictive power\n",
    " - Rate_%: this is our target variable\n",
    " - Effective_Rate_%: this is similar to the above but expressed in a different manner [taking into the effect of compounding on a periodic basis]. This can be confirmed referring back to the pairplots between Rate_% and Effective_Rate_%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_summary.drop(['Note_ID', 'Funded_%', 'Rate_%', 'Effective_Rate_%'], axis = 1)\n",
    "y = df_summary['Rate_%']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate a basic correlation matrix to identify any features that are highly correlated to each other. We then drop them to avoid any issues of multicollinearity in our regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprise that the PD_% (probability of default) correlates highly with the Grade. The higher the grade, the higher the PD_%. Let's drop the Grade feature then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('Grade', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "lr_base = LinearRegression()\n",
    "lr_base.fit(X_train, y_train)\n",
    "base_R2_score = round(lr_base.score(X_train, y_train), 4)\n",
    "print ('Base R2 score of:', base_R2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does pretty poorly out of the box at 56% R2. Let's try exploring interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_interact_1 = X_train.copy()\n",
    "X_interact_2 = X_train.copy()\n",
    "X_interact_3 = X_train.copy()\n",
    "\n",
    "X_interact_1['Term_PD_%'] = X_interact_1['Term'] * X_interact_1['PD_%']\n",
    "X_interact_2['Term_Funding'] = X_interact_2['Term'] * X_interact_2['Funding']\n",
    "X_interact_3['Funding_PD_%'] = X_interact_3['Funding'] * X_interact_3['PD_%']\n",
    "\n",
    "models_to_evaluate = [X_interact_1, X_interact_2, X_interact_3]\n",
    "\n",
    "for model in models_to_evaluate:\n",
    "    interact = LinearRegression()\n",
    "    interact.fit(model, y_train)\n",
    "    interact_score = interact.score(model, y_train)\n",
    "    print ('R2 of: {}'.format(round(interact_score, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like introducing interaction terms do not improve the model performance by a significant amount. Maybe polynomials will help? Let's take a look again at the scatter plots for each of our independent variables against the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (25, 5), sharey = True)\n",
    "ax1.scatter(df_summary['Funding'], df_summary['Rate_%'])\n",
    "ax1.set_title('Funding vs Rate %')\n",
    "ax2.scatter(df_summary['Term'], df_summary['Rate_%'])\n",
    "ax2.set_title('Tenor vs Rate %')\n",
    "ax3.scatter(df_summary['PD_%'], df_summary['Rate_%'])\n",
    "ax3.set_title('PD % vs Rate %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at the scatterplots, it is fairly difficult to deduct if there are any non-linear relationships. There could possibly be some form of quadratic relationship for both the funding and PD_% features against the target variable. Let's introduce them and see if there are any improvements in the model's R2 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly_2 = X_train.copy()\n",
    "\n",
    "X_train_poly_2['Funding2'] = X_train_poly_2['Funding'] ** 2\n",
    "X_train_poly_2['PD2'] = X_train_poly_2['PD_%'] ** 2\n",
    "\n",
    "poly = LinearRegression()\n",
    "poly.fit(X_train_poly_2, y_train)\n",
    "poly_R2_score = round(poly.score(X_train_poly_2, y_train), 4)\n",
    "\n",
    "print ('Poly R2 score of:', poly_R2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is a 5% improvement in our model just by introducing a quadratic relationship in two of the independent features. Now, let's also explore other types of regressor model out there to examine if we can improve upon the 61% obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_range = range(2,26)\n",
    "cv_scores = []\n",
    "\n",
    "for i in num_range:\n",
    "    poly_cv = LinearRegression()\n",
    "    cv_scores.append(np.mean(cross_val_score(poly_cv, X_train_poly_2, y_train, cv = i)))\n",
    "    \n",
    "plt.plot(num_range, cv_scores)\n",
    "plt.ylabel('CV R2 Mean Scores')\n",
    "plt.xlabel('KFolds')\n",
    "plt.title('Evolution of CV R2 scores against number of K-Folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this through Statsmodel's OLS function to get a full summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "\n",
    "linreg_poly = OLS(y_train, add_constant(X_train_poly_2)).fit()\n",
    "linreg_poly.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "params_grid = {'max_depth' : [1, 2, 3, 4, 5, 6, 7],\n",
    "                 'min_samples_leaf' : range(5,50,5),\n",
    "                 'max_features': [1, 2, 3]}\n",
    "\n",
    "rf_grid = RandomForestRegressor()\n",
    "grid_rf = GridSearchCV(rf_grid, param_grid = params_grid, n_jobs = -1, verbose = True, cv = 10)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print (round(grid_rf.best_score_, 4))\n",
    "print (grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "\n",
    "num_range = range(2,26)\n",
    "cv_scores_rf = []\n",
    "\n",
    "for i in pbar(num_range):\n",
    "    rf_regressor_cv = RandomForestRegressor(n_jobs=-1, max_depth = 6, max_features = 2, min_samples_leaf = 5)\n",
    "    cv_scores_rf.append(np.mean(cross_val_score(rf_regressor_cv, X_train, y_train, cv = i)))\n",
    "    \n",
    "plt.plot(num_range, cv_scores_rf)\n",
    "plt.ylabel('CV R2 Mean Scores')\n",
    "plt.xlabel('KFolds')\n",
    "plt.title('Evolution of CV R2 scores against number of K-Folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, looks like we've improved R2 score by a fairly significant amount (ie 8%). Let's now test our chosen RF regressor model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regressor_cv.fit(X_train, y_train)\n",
    "predictions =  rf_regressor_cv.predict(X_test)\n",
    "round(r2_score(y_test, predictions), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now deploy our model to guide future investments. For fun, let's apply this to the past investments and also specify a 50bps threshold (accept any investments which are priced 50bps above our model price)| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    subset = x[['Term', 'PD_%', 'Funding']].values.reshape(1, -1)\n",
    "    prediction = rf_regressor_cv.predict(subset)\n",
    "    return round(prediction[0], 2)\n",
    "\n",
    "def invest_decision(x):\n",
    "    if x['Delta_%'] >= 0.5:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary['Predicted_Rate_%'] = df_summary.apply(predict, axis = 1)\n",
    "df_summary['Delta_%'] = df_summary['Rate_%'] - df_summary['Predicted_Rate_%']\n",
    "df_summary['Invest'] = df_summary.apply(invest_decision, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.head(5).drop(['Effective_Rate_%', 'Funded_%'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_invest = df_summary[df_summary.Invest == 'Yes'].drop('Invest', axis = 1).reset_index(drop = True)\n",
    "yes_invest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Recommendation Engine using Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler()\n",
    "yes_invest_scaled = mm.fit_transform(yes_invest.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([yes_invest.Note_ID, pd.DataFrame(yes_invest_scaled, columns = yes_invest.columns[1:])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(x):\n",
    "    b = x[['Grade', 'Rate_%', 'Effective_Rate_%', \n",
    "           'Term', 'PD_%','Funding', 'Funded_%', 'Predicted_Rate_%', 'Delta_%']].values\n",
    "    \n",
    "    dot = np.dot(chosen_array, b)\n",
    "    norm_chosen = np.linalg.norm(chosen_array)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    cos = dot / (norm_chosen * norm_b)\n",
    "    \n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_id = 1115\n",
    "\n",
    "test['Cosine'] = test.apply(cosine, axis = 1)\n",
    "recommendation_id = test.sort_values(by = 'Cosine', ascending = False)[1:6]['Note_ID'].values\n",
    "\n",
    "print ('Chosen ID:')\n",
    "display(df_summary[df_summary.Note_ID == chosen_id].drop(['Effective_Rate_%', 'Funded_%'], axis = 1))\n",
    "\n",
    "print ('Top 5 similar IDs to chosen ID')\n",
    "df_summary[df_summary.Note_ID.isin(recommendation_id)].drop(['Effective_Rate_%', 'Funded_%'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "999999999999999999999999999999999999999999999oool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
